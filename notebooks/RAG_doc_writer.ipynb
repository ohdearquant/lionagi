{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lionagi==0.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path.cwd() / \"lionagi_data\"  # Path to the data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files from directory\n",
    "docs = li.load(\n",
    "    input_dir=data_path, \n",
    "    recursive=True, \n",
    "    required_exts=[\".py\"]\n",
    ")\n",
    "\n",
    "docs = [doc for doc in docs if len(doc.content) > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chunk\n",
    "# pile = li.chunk(docs=docs, chunk_size=2000, overlap=0.1)\n",
    "\n",
    "# # embed\n",
    "\n",
    "# embed_model = li.iModel(\n",
    "#     model=\"text-embedding-3-small\", \n",
    "#     provider=\"openai\",\n",
    "#     endpoint=\"embeddings\",\n",
    "#     interval_tokens=2_000_000,\n",
    "#     interval_requests=10_000,\n",
    "#     interval=60,\n",
    "#     # api_key = \"YOUR_API_KEY_HERE\" # Optional if you already put it in .env file\n",
    "# )\n",
    "\n",
    "# await pile.embed_pile(imodel=embed_model)\n",
    "\n",
    "# # save\n",
    "# pile.to_csv(\"lionagi_embedding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload pile from saved csv\n",
    "pile = li.pile(csv_file=\"lionagi_embedding.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "write a good API documentation for provided code, must use \n",
    "query engine to check meanings of related code concepts \n",
    "to accurately describe, for example if a name of a variable,\n",
    "function, class, or module is used but not present in context,\n",
    "- You MUST check with the query engine. \n",
    "- You MUST use extension at least ONCE. \n",
    "- You MUST make sure that documentation is accurate and detailed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi.libs.sys_util import SysUtil\n",
    "SysUtil.check_import(\n",
    "    package_name=\"llama_index\", \n",
    "    module_name=\"core.postprocessor\",\n",
    "    import_name=\"LLMRerank\",\n",
    "    pip_name=\"llama-index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "reranker = LLMRerank(\n",
    "    choice_batch_size=10,\n",
    "    top_n=4\n",
    ")\n",
    "\n",
    "tools = pile.as_query_tool(\n",
    "    name=\"qa_lionagi\",\n",
    "    guidance=\"Perform query to a QA bot with access to lionagi codebase\",\n",
    "    query_description=\"a term/phrase/question to lookup or a question to answer\",\n",
    "    node_postprocessors = [reranker] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An imodel in lionagi is not explicitly defined in the provided information.\n"
     ]
    }
   ],
   "source": [
    "print(await pile.query_pile(\"what is a imodel in lionagi?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatting with model...\n",
      "Found action requests in model response. Processing actions...\n",
      "Actions processed!\n",
      "\n",
      "Found extension requests in model response.\n",
      "------------------- Processing extension No.1 -------------------\n",
      "Chatting with model...\n",
      "Found action requests in model response. Processing actions...\n",
      "Actions processed!\n",
      "Analyzing action responses and generating answer...\n",
      "------------------- Extension completed -------------------\n",
      "\n",
      "Analyzing action responses and generating answer...\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Directive successfully completed!\n"
     ]
    }
   ],
   "source": [
    "from PROMPTS import sys_prompt  # put your system prompt here\n",
    "\n",
    "model = li.iModel(\n",
    "    model=\"gpt-4o\",\n",
    "    provider=\"openai\",\n",
    "    interval_tokens=1_000_000,\n",
    "    interval_requests=1_000,\n",
    "    interval=60,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "branch = li.Branch(system=sys_prompt, tools=tools, imodel=model)\n",
    "\n",
    "form = await branch.direct(\n",
    "    instruction=instruction,\n",
    "    context=docs[249].content + docs[250].content,\n",
    "    allow_action=True,\n",
    "    allow_extension=True,\n",
    "    verbose=True,\n",
    "    max_extensions=2,\n",
    "    retries=3,  # sometimes the model may fail to generate a valid response or refuse to take actions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ln_id</th>\n",
       "      <th>message_type</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>role</th>\n",
       "      <th>content</th>\n",
       "      <th>metadata</th>\n",
       "      <th>sender</th>\n",
       "      <th>recipient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8586b678044ac8903aaebce8564f80d1</td>\n",
       "      <td>System</td>\n",
       "      <td>2024-08-02T17:04:29.961993</td>\n",
       "      <td>system</td>\n",
       "      <td>{'system_info': '\n",
       "you are a helpful assistant,...</td>\n",
       "      <td>{'last_updated': {'recipient': '2024-08-02T17:...</td>\n",
       "      <td>system</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03be5d692a1ee8c0b6b48da0a4e3a23b</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>2024-08-02T17:04:29.962882</td>\n",
       "      <td>user</td>\n",
       "      <td>{'instruction': '\n",
       "        ## Task Instructions...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-08-02T17:04:...</td>\n",
       "      <td>user</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71e2f697fa2bac8c32faf4775a81cf7c</td>\n",
       "      <td>AssistantResponse</td>\n",
       "      <td>2024-08-02T17:04:36.541177</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'assistant_response': '```json\n",
       "{\n",
       "    \"answer\"...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-08-02T17:04:...</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fe6bd5eae3f1a76c79377de2aba71576</td>\n",
       "      <td>ActionRequest</td>\n",
       "      <td>2024-08-02T17:04:36.544916</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_request': {'function': 'qa_lionagi', ...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>952cf034c59873827d968dcd6500d4f4</td>\n",
       "      <td>ActionRequest</td>\n",
       "      <td>2024-08-02T17:04:36.545062</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_request': {'function': 'qa_lionagi', ...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44a9fc9848ae8847cb992f964ad7b3e6</td>\n",
       "      <td>ActionRequest</td>\n",
       "      <td>2024-08-02T17:04:36.545131</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_request': {'function': 'qa_lionagi', ...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5349939fc0b4204f67726b97b3e44011</td>\n",
       "      <td>ActionResponse</td>\n",
       "      <td>2024-08-02T17:04:38.152338</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_response': {'function': 'qa_lionagi',...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dafef79aa3a08481cfab895824b0a27d</td>\n",
       "      <td>ActionResponse</td>\n",
       "      <td>2024-08-02T17:04:38.153030</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_response': {'function': 'qa_lionagi',...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3fb77df516c9a95e2cfb52cf9189d7f7</td>\n",
       "      <td>ActionResponse</td>\n",
       "      <td>2024-08-02T17:04:38.153318</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_response': {'function': 'qa_lionagi',...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>970f93ba51143f775bd04ffbfeba1dad</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>2024-08-02T17:04:38.155275</td>\n",
       "      <td>user</td>\n",
       "      <td>{'instruction': '\n",
       "        ## Task Instructions...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-08-02T17:04:...</td>\n",
       "      <td>user</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1ede63771dd2356c4bf68f3910afb274</td>\n",
       "      <td>AssistantResponse</td>\n",
       "      <td>2024-08-02T17:04:42.813905</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'assistant_response': '```json\n",
       "{\n",
       "    \"answer\"...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-08-02T17:04:...</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3d0bc321f35ec09e97764346a91c31e0</td>\n",
       "      <td>ActionRequest</td>\n",
       "      <td>2024-08-02T17:04:42.817223</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_request': {'function': 'qa_lionagi', ...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b8cac8d552f2ce9925aa57ef89934bff</td>\n",
       "      <td>ActionRequest</td>\n",
       "      <td>2024-08-02T17:04:42.817537</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_request': {'function': 'qa_lionagi', ...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b5259d1460fcdd19d4613fdce6b89935</td>\n",
       "      <td>ActionRequest</td>\n",
       "      <td>2024-08-02T17:04:42.817803</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_request': {'function': 'qa_lionagi', ...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fe4d9083bcadac9cf45a8a586ce837d3</td>\n",
       "      <td>ActionResponse</td>\n",
       "      <td>2024-08-02T17:04:44.709517</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_response': {'function': 'qa_lionagi',...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7ae76714b0ef5db3fe0f463de9998657</td>\n",
       "      <td>ActionResponse</td>\n",
       "      <td>2024-08-02T17:04:44.709931</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_response': {'function': 'qa_lionagi',...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>141839480714677f4c36e0bd72abe648</td>\n",
       "      <td>ActionResponse</td>\n",
       "      <td>2024-08-02T17:04:44.710170</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'action_response': {'function': 'qa_lionagi',...</td>\n",
       "      <td>{'last_updated': {'function': '2024-08-02T17:0...</td>\n",
       "      <td>dbd791a9b37750318430bfe4cc306959</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>317998e63da7dd5d061e7b9212d146b4</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>2024-08-02T17:04:44.711033</td>\n",
       "      <td>user</td>\n",
       "      <td>{'instruction': 'please provide final answer b...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-08-02T17:04:...</td>\n",
       "      <td>user</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ced3413f2f5b2eaa6e6c2d7bbe42dbb3</td>\n",
       "      <td>AssistantResponse</td>\n",
       "      <td>2024-08-02T17:04:48.279220</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'assistant_response': 'The provided code cont...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-08-02T17:04:...</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>54d042b096b9c640469baaecb324dbc5</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>2024-08-02T17:04:48.279766</td>\n",
       "      <td>user</td>\n",
       "      <td>{'instruction': 'please provide final answer b...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-08-02T17:04:...</td>\n",
       "      <td>user</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>e7217a4380809991f4cfe000c61f1d26</td>\n",
       "      <td>AssistantResponse</td>\n",
       "      <td>2024-08-02T17:04:51.457150</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'assistant_response': 'The provided code cont...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-08-02T17:04:...</td>\n",
       "      <td>876cc2a83dbc7bdab58d82169119e545</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ln_id       message_type  \\\n",
       "0   8586b678044ac8903aaebce8564f80d1             System   \n",
       "1   03be5d692a1ee8c0b6b48da0a4e3a23b        Instruction   \n",
       "2   71e2f697fa2bac8c32faf4775a81cf7c  AssistantResponse   \n",
       "3   fe6bd5eae3f1a76c79377de2aba71576      ActionRequest   \n",
       "4   952cf034c59873827d968dcd6500d4f4      ActionRequest   \n",
       "5   44a9fc9848ae8847cb992f964ad7b3e6      ActionRequest   \n",
       "6   5349939fc0b4204f67726b97b3e44011     ActionResponse   \n",
       "7   dafef79aa3a08481cfab895824b0a27d     ActionResponse   \n",
       "8   3fb77df516c9a95e2cfb52cf9189d7f7     ActionResponse   \n",
       "9   970f93ba51143f775bd04ffbfeba1dad        Instruction   \n",
       "10  1ede63771dd2356c4bf68f3910afb274  AssistantResponse   \n",
       "11  3d0bc321f35ec09e97764346a91c31e0      ActionRequest   \n",
       "12  b8cac8d552f2ce9925aa57ef89934bff      ActionRequest   \n",
       "13  b5259d1460fcdd19d4613fdce6b89935      ActionRequest   \n",
       "14  fe4d9083bcadac9cf45a8a586ce837d3     ActionResponse   \n",
       "15  7ae76714b0ef5db3fe0f463de9998657     ActionResponse   \n",
       "16  141839480714677f4c36e0bd72abe648     ActionResponse   \n",
       "17  317998e63da7dd5d061e7b9212d146b4        Instruction   \n",
       "18  ced3413f2f5b2eaa6e6c2d7bbe42dbb3  AssistantResponse   \n",
       "19  54d042b096b9c640469baaecb324dbc5        Instruction   \n",
       "20  e7217a4380809991f4cfe000c61f1d26  AssistantResponse   \n",
       "\n",
       "                     timestamp       role  \\\n",
       "0   2024-08-02T17:04:29.961993     system   \n",
       "1   2024-08-02T17:04:29.962882       user   \n",
       "2   2024-08-02T17:04:36.541177  assistant   \n",
       "3   2024-08-02T17:04:36.544916  assistant   \n",
       "4   2024-08-02T17:04:36.545062  assistant   \n",
       "5   2024-08-02T17:04:36.545131  assistant   \n",
       "6   2024-08-02T17:04:38.152338  assistant   \n",
       "7   2024-08-02T17:04:38.153030  assistant   \n",
       "8   2024-08-02T17:04:38.153318  assistant   \n",
       "9   2024-08-02T17:04:38.155275       user   \n",
       "10  2024-08-02T17:04:42.813905  assistant   \n",
       "11  2024-08-02T17:04:42.817223  assistant   \n",
       "12  2024-08-02T17:04:42.817537  assistant   \n",
       "13  2024-08-02T17:04:42.817803  assistant   \n",
       "14  2024-08-02T17:04:44.709517  assistant   \n",
       "15  2024-08-02T17:04:44.709931  assistant   \n",
       "16  2024-08-02T17:04:44.710170  assistant   \n",
       "17  2024-08-02T17:04:44.711033       user   \n",
       "18  2024-08-02T17:04:48.279220  assistant   \n",
       "19  2024-08-02T17:04:48.279766       user   \n",
       "20  2024-08-02T17:04:51.457150  assistant   \n",
       "\n",
       "                                              content  \\\n",
       "0   {'system_info': '\n",
       "you are a helpful assistant,...   \n",
       "1   {'instruction': '\n",
       "        ## Task Instructions...   \n",
       "2   {'assistant_response': '```json\n",
       "{\n",
       "    \"answer\"...   \n",
       "3   {'action_request': {'function': 'qa_lionagi', ...   \n",
       "4   {'action_request': {'function': 'qa_lionagi', ...   \n",
       "5   {'action_request': {'function': 'qa_lionagi', ...   \n",
       "6   {'action_response': {'function': 'qa_lionagi',...   \n",
       "7   {'action_response': {'function': 'qa_lionagi',...   \n",
       "8   {'action_response': {'function': 'qa_lionagi',...   \n",
       "9   {'instruction': '\n",
       "        ## Task Instructions...   \n",
       "10  {'assistant_response': '```json\n",
       "{\n",
       "    \"answer\"...   \n",
       "11  {'action_request': {'function': 'qa_lionagi', ...   \n",
       "12  {'action_request': {'function': 'qa_lionagi', ...   \n",
       "13  {'action_request': {'function': 'qa_lionagi', ...   \n",
       "14  {'action_response': {'function': 'qa_lionagi',...   \n",
       "15  {'action_response': {'function': 'qa_lionagi',...   \n",
       "16  {'action_response': {'function': 'qa_lionagi',...   \n",
       "17  {'instruction': 'please provide final answer b...   \n",
       "18  {'assistant_response': 'The provided code cont...   \n",
       "19  {'instruction': 'please provide final answer b...   \n",
       "20  {'assistant_response': 'The provided code cont...   \n",
       "\n",
       "                                             metadata  \\\n",
       "0   {'last_updated': {'recipient': '2024-08-02T17:...   \n",
       "1   {'last_updated': {'sender': '2024-08-02T17:04:...   \n",
       "2   {'last_updated': {'sender': '2024-08-02T17:04:...   \n",
       "3   {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "4   {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "5   {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "6   {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "7   {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "8   {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "9   {'last_updated': {'sender': '2024-08-02T17:04:...   \n",
       "10  {'last_updated': {'sender': '2024-08-02T17:04:...   \n",
       "11  {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "12  {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "13  {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "14  {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "15  {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "16  {'last_updated': {'function': '2024-08-02T17:0...   \n",
       "17  {'last_updated': {'sender': '2024-08-02T17:04:...   \n",
       "18  {'last_updated': {'sender': '2024-08-02T17:04:...   \n",
       "19  {'last_updated': {'sender': '2024-08-02T17:04:...   \n",
       "20  {'last_updated': {'sender': '2024-08-02T17:04:...   \n",
       "\n",
       "                              sender                         recipient  \n",
       "0                             system  876cc2a83dbc7bdab58d82169119e545  \n",
       "1                               user  876cc2a83dbc7bdab58d82169119e545  \n",
       "2   876cc2a83dbc7bdab58d82169119e545                              user  \n",
       "3   876cc2a83dbc7bdab58d82169119e545  dbd791a9b37750318430bfe4cc306959  \n",
       "4   876cc2a83dbc7bdab58d82169119e545  dbd791a9b37750318430bfe4cc306959  \n",
       "5   876cc2a83dbc7bdab58d82169119e545  dbd791a9b37750318430bfe4cc306959  \n",
       "6   dbd791a9b37750318430bfe4cc306959  876cc2a83dbc7bdab58d82169119e545  \n",
       "7   dbd791a9b37750318430bfe4cc306959  876cc2a83dbc7bdab58d82169119e545  \n",
       "8   dbd791a9b37750318430bfe4cc306959  876cc2a83dbc7bdab58d82169119e545  \n",
       "9                               user  876cc2a83dbc7bdab58d82169119e545  \n",
       "10  876cc2a83dbc7bdab58d82169119e545                              user  \n",
       "11  876cc2a83dbc7bdab58d82169119e545  dbd791a9b37750318430bfe4cc306959  \n",
       "12  876cc2a83dbc7bdab58d82169119e545  dbd791a9b37750318430bfe4cc306959  \n",
       "13  876cc2a83dbc7bdab58d82169119e545  dbd791a9b37750318430bfe4cc306959  \n",
       "14  dbd791a9b37750318430bfe4cc306959  876cc2a83dbc7bdab58d82169119e545  \n",
       "15  dbd791a9b37750318430bfe4cc306959  876cc2a83dbc7bdab58d82169119e545  \n",
       "16  dbd791a9b37750318430bfe4cc306959  876cc2a83dbc7bdab58d82169119e545  \n",
       "17                              user  876cc2a83dbc7bdab58d82169119e545  \n",
       "18  876cc2a83dbc7bdab58d82169119e545                              user  \n",
       "19                              user  876cc2a83dbc7bdab58d82169119e545  \n",
       "20  876cc2a83dbc7bdab58d82169119e545                              user  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**task**: \n",
       " Follow the prompt and provide the necessary output.\n",
       "- Additional instruction: \n",
       "write a good API documentation for provided code, must use \n",
       "query engine to check meanings of related code concepts \n",
       "to accurately describe, for example if a name of a variable,\n",
       "function, class, or module is used but not present in context,\n",
       "- You MUST check with the query engine. \n",
       "- You MUST use extension at least ONCE. \n",
       "- You MUST make sure that documentation is accurate and detailed.\n",
       "\n",
       "- Additional context: import asyncio\n",
       "import numpy as np\n",
       "from typing import Callable\n",
       "from lion_core.libs import to_list, nget\n",
       "from lionagi.os.operator.imodel.imodel import iModel\n",
       "\n",
       "async def compute_perplexity(\n",
       "    imodel: iModel,\n",
       "    initial_context: str = None,\n",
       "    tokens: list[str] = None,\n",
       "    system_msg: str = None,\n",
       "    n_samples: int = 1,  # number of samples used for the computation\n",
       "    use_residue: bool = True,  # whether to use residue for the last sample\n",
       "    **kwargs,  # additional arguments for the model\n",
       ") -> tuple[list[str], float]:\n",
       "    tasks = []\n",
       "    context = initial_context or \"\"\n",
       "    n_samples = n_samples or len(tokens)\n",
       "    sample_token_len, residue = divmod(len(tokens), n_samples)\n",
       "    samples = []\n",
       "    if n_samples == 1:\n",
       "        samples = [tokens]\n",
       "    else:\n",
       "        samples = [tokens[: (i + 1) * sample_token_len] for i in range(n_samples)]\n",
       "        if use_residue and residue != 0:\n",
       "            samples.append(tokens[-residue:])\n",
       "    sampless = [context + \" \".join(sample) for sample in samples]\n",
       "    for sample in sampless:\n",
       "        messages = [{\"role\": \"system\", \"content\": system_msg}] if system_msg else []\n",
       "        messages.append(\n",
       "            {\"role\": \"user\", \"content\": sample},\n",
       "        )\n",
       "        task = asyncio.create_task(\n",
       "            imodel.chat_completion(\n",
       "                messages=messages,\n",
       "                logprobs=True,\n",
       "                max_tokens=sample_token_len,\n",
       "                **kwargs,\n",
       "            )\n",
       "        )\n",
       "        tasks.append(task)\n",
       "    results = await asyncio.gather(*tasks)  # result is (p..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**reason**: \n",
       " Let's think step by step. To provide accurate and detailed API documentation, it is essential to understand the meanings of the variables, functions, and classes used in the code. The terms 'nget', 'to_list', and 'iModel' are not explicitly defined in the provided context, so querying their meanings will ensure the documentation is precise and comprehensive."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**actions**: \n",
       " \n",
       " \n",
       "1. **qa_lionagi**(query: Meaning of the variable 'nget' in the provided code context.), \n",
       " \n",
       "2. **qa_lionagi**(query: Meaning of the function 'to_list' in the provided code context.), \n",
       " \n",
       "3. **qa_lionagi**(query: Meaning of the class 'iModel' in the provided code context.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**action_required**: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**extension_required**: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**action_response**: \n",
       " \n",
       " \n",
       "1. **qa_lionagi**(query: Meaning of the function 'to_list' in the provided code context.) \n",
       " \n",
       " The function 'to_list' in the provided code context is used to convert the 'indices' parameter into a list format. This ensures that the indices can be properly processed and navigated through the nested structure., \n",
       " \n",
       "2. **qa_lionagi**(query: Meaning of the class 'iModel' in the provided code context.) \n",
       " \n",
       " The provided code context does not include a class named 'iModel'. Therefore, it is not possible to determine the meaning or functionality of a class by that name from the given information., \n",
       " \n",
       "3. **qa_lionagi**(query: Meaning of the variable 'nget' in the provided code context.) \n",
       " \n",
       " The provided code context does not contain any reference to a variable named 'nget'. Therefore, it is not possible to determine the meaning of 'nget' from the given information., \n",
       " \n",
       "4. **qa_lionagi**(query: Meaning of the variable 'nget' in the provided code context.) \n",
       " \n",
       " The provided code context does not include a variable named 'nget'. Therefore, it is not possible to determine the meaning of 'nget' from the given information., \n",
       " \n",
       "5. **qa_lionagi**(query: Meaning of the function 'to_list' in the provided code context.) \n",
       " \n",
       " The function `to_list` in the provided code context is used to convert the `indices` parameter into a list format. This ensures that the subsequent operations on `indices` can be performed consistently, regardless of the initial type of `indices`., \n",
       " \n",
       "6. **qa_lionagi**(query: Meaning of the class 'iModel' in the provided code context.) \n",
       " \n",
       " The provided code context does not contain any information about a class named 'iModel'. Therefore, it is not possible to determine the meaning or details of the class 'iModel' from the given information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**action_performed**: True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**available_tools**: ['qa_lionagi']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**answer**: \n",
       " The provided code context does not contain any reference to a variable named 'nget'. Therefore, it is not possible to determine the meaning of 'nget' from the given information. The function `to_list` in the provided code context is used to convert the `indices` parameter into a list format. This ensures that the subsequent operations on `indices` can be performed consistently, regardless of the initial type of `indices`. The provided code context does not contain any information about a class named 'iModel'. Therefore, it is not possible to determine the meaning or details of the class 'iModel' from the given information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "form.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit = \"\"\"\n",
    "provide documentation only: final documentation in md \n",
    "format of the module of interest, do not include other \n",
    "fields do not present in JSON format, only markdown format \n",
    "you asked a lot of good questions and got plenty answers, \n",
    "please integrate your conversation, be a lot more technical, you will \n",
    "be rewarded with 500 dollars for great work, and \n",
    "punished for subpar work, take a deep breath, you can do it\n",
    "\"\"\"\n",
    "\n",
    "final_output = await branch.chat(instruction=edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# API Documentation\n",
       "\n",
       "## Module: Perplexity Computation and Token Selection\n",
       "\n",
       "### Class: `iModel`\n",
       "**Description**: \n",
       "`iModel` is an interface for interacting with language models. It provides methods for generating text completions and computing log probabilities of tokens.\n",
       "\n",
       "### Function: `compute_perplexity`\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "async def compute_perplexity(\n",
       "    imodel: iModel,\n",
       "    initial_context: str = None,\n",
       "    tokens: list[str] = None,\n",
       "    system_msg: str = None,\n",
       "    n_samples: int = 1,\n",
       "    use_residue: bool = True,\n",
       "    **kwargs\n",
       ") -> tuple[list[str], float]:\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `imodel` (iModel): An instance of the `iModel` class used to generate text completions.\n",
       "- `initial_context` (str, optional): The initial context string to prepend to the tokens. Default is `None`.\n",
       "- `tokens` (list[str], optional): A list of tokens for which to compute perplexity. Default is `None`.\n",
       "- `system_msg` (str, optional): A system message to include in the context. Default is `None`.\n",
       "- `n_samples` (int, optional): The number of samples to use for the computation. Default is `1`.\n",
       "- `use_residue` (bool, optional): Whether to use the residue for the last sample. Default is `True`.\n",
       "- `**kwargs`: Additional arguments for the model.\n",
       "\n",
       "**Return Values**:\n",
       "- `tuple[list[str], float]`: A tuple containing the list of tokens and the computed perplexity.\n",
       "\n",
       "**Exceptions Raised**:\n",
       "- `ValueError`: Raised if `n_samples` is less than 1.\n",
       "\n",
       "**Usage Examples**:\n",
       "```python\n",
       "# Example 1: Basic usage with required parameters\n",
       "result = await compute_perplexity(imodel, tokens=[\"hello\", \"world\"])\n",
       "print(result)\n",
       "\n",
       "# Example 2: Usage with initial context and system message\n",
       "result = await compute_perplexity(imodel, initial_context=\"This is a test.\", tokens=[\"hello\", \"world\"], system_msg=\"System message\")\n",
       "print(result)\n",
       "\n",
       "# Example 3: Handling exceptions\n",
       "try:\n",
       "    result = await compute_perplexity(imodel, tokens=[\"hello\", \"world\"], n_samples=0)\n",
       "except ValueError as e:\n",
       "    print(e)  # Output: n_samples must be at least 1\n",
       "```\n",
       "\n",
       "### Function: `select_by_pplex`\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "def select_by_pplex(\n",
       "    ranked_items: list,\n",
       "    target_compression_ratio: float,\n",
       "    original_length: int,\n",
       "    tokenizer: Callable,\n",
       "    min_pplex: float,\n",
       ") -> list:\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `ranked_items` (list): A list of ranked items based on perplexity.\n",
       "- `target_compression_ratio` (float): The target compression ratio.\n",
       "- `original_length` (int): The original length of the text.\n",
       "- `tokenizer` (Callable): A callable used to tokenize the text.\n",
       "- `min_pplex` (float, optional): The minimum perplexity threshold. Default is `0`.\n",
       "\n",
       "**Return Values**:\n",
       "- `list`: A list of selected items based on the target compression ratio and minimum perplexity.\n",
       "\n",
       "**Usage Examples**:\n",
       "```python\n",
       "# Example 1: Basic usage with required parameters\n",
       "selected_items = select_by_pplex(ranked_items, 0.5, 100, tokenizer, 0.1)\n",
       "print(selected_items)\n",
       "\n",
       "# Example 2: Usage with different compression ratio\n",
       "selected_items = select_by_pplex(ranked_items, 0.3, 100, tokenizer, 0.1)\n",
       "print(selected_items)\n",
       "```\n",
       "\n",
       "### Helper Functions\n",
       "\n",
       "#### Function: `to_list`\n",
       "\n",
       "**Description**: \n",
       "Converts an input to a list, optionally flattening nested lists and dropping `None` values.\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "def to_list(input, flatten=False, dropna=False) -> list:\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `input`: The input to convert to a list.\n",
       "- `flatten` (bool, optional): Whether to flatten nested lists. Default is `False`.\n",
       "- `dropna` (bool, optional): Whether to drop `None` values. Default is `False`.\n",
       "\n",
       "**Return Values**:\n",
       "- `list`: The converted list.\n",
       "\n",
       "#### Function: `nget`\n",
       "\n",
       "**Description**: \n",
       "Safely retrieves a nested value from a dictionary.\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "def nget(keys, dictionary, default=None):\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `keys` (list): A list of keys to traverse the dictionary.\n",
       "- `dictionary` (dict): The dictionary to retrieve the value from.\n",
       "- `default`: The default value to return if the key is not found.\n",
       "\n",
       "**Return Values**:\n",
       "- The retrieved value or the default value if the key is not found.\n",
       "\n",
       "### Utility Functions\n",
       "\n",
       "#### Function: `chunk_by_chars`\n",
       "\n",
       "**Description**: \n",
       "Chunks text by characters.\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "def chunk_by_chars(text, chunk_size, overlap=0, threshold=0, return_tokens=False, return_byte=False):\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `text` (str): The text to chunk.\n",
       "- `chunk_size` (int): The size of each chunk.\n",
       "- `overlap` (int, optional): The number of overlapping characters between chunks. Default is `0`.\n",
       "- `threshold` (int, optional): The threshold for chunking. Default is `0`.\n",
       "- `return_tokens` (bool, optional): Whether to return tokens. Default is `False`.\n",
       "- `return_byte` (bool, optional): Whether to return bytes. Default is `False`.\n",
       "\n",
       "**Return Values**:\n",
       "- The chunked text.\n",
       "\n",
       "#### Function: `chunk_by_tokens`\n",
       "\n",
       "**Description**: \n",
       "Chunks text by tokens.\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "def chunk_by_tokens(text, chunk_size, overlap=0, threshold=0, return_tokens=False, return_byte=False):\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `text` (str): The text to chunk.\n",
       "- `chunk_size` (int): The size of each chunk.\n",
       "- `overlap` (int, optional): The number of overlapping tokens between chunks. Default is `0`.\n",
       "- `threshold` (int, optional): The threshold for chunking. Default is `0`.\n",
       "- `return_tokens` (bool, optional): Whether to return tokens. Default is `False`.\n",
       "- `return_byte` (bool, optional): Whether to return bytes. Default is `False`.\n",
       "\n",
       "**Return Values**:\n",
       "- The chunked text.\n",
       "\n",
       "#### Function: `tokenize`\n",
       "\n",
       "**Description**: \n",
       "Tokenizes text using a specified encoding model.\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "def tokenize(text, encoding_model, encoding_name=None, return_byte=False):\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `text` (str): The text to tokenize.\n",
       "- `encoding_model` (str): The encoding model to use.\n",
       "- `encoding_name` (str, optional): The name of the encoding. Default is `None`.\n",
       "- `return_byte` (bool, optional): Whether to return bytes. Default is `False`.\n",
       "\n",
       "**Return Values**:\n",
       "- The tokenized text.\n",
       "\n",
       "### Class: `LLMCompressor`\n",
       "\n",
       "**Description**: \n",
       "`LLMCompressor` is a class for compressing text using language models. It provides methods for tokenizing, splitting, and compressing text based on perplexity.\n",
       "\n",
       "**Constructor**:\n",
       "```python\n",
       "def __init__(\n",
       "    self,\n",
       "    imodel: iModel = None,\n",
       "    system_msg=None,\n",
       "    tokenizer=None,\n",
       "    splitter=None,\n",
       "    target_ratio=0.2,\n",
       "    n_samples=5,\n",
       "    chunk_size=64,\n",
       "    max_tokens_per_sample=80,\n",
       "    min_compression_score=0,\n",
       "    split_overlap=0,\n",
       "    split_threshold=0,\n",
       "    verbose=True,\n",
       "):\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `imodel` (iModel, optional): An instance of the `iModel` class. Default is `None`.\n",
       "- `system_msg` (str, optional): A system message for the model. Default is `None`.\n",
       "- `tokenizer` (Callable, optional): A callable for tokenizing text. Default is `None`.\n",
       "- `splitter` (Callable, optional): A callable for splitting text. Default is `None`.\n",
       "- `target_ratio` (float, optional): The target compression ratio. Default is `0.2`.\n",
       "- `n_samples` (int, optional): The number of samples for perplexity calculation. Default is `5`.\n",
       "- `chunk_size` (int, optional): The size of each chunk. Default is `64`.\n",
       "- `max_tokens_per_sample` (int, optional): The maximum number of tokens per sample. Default is `80`.\n",
       "- `min_compression_score` (float, optional): The minimum compression score. Default is `0`.\n",
       "- `split_overlap` (int, optional): The number of overlapping tokens between chunks. Default is `0`.\n",
       "- `split_threshold` (int, optional): The threshold for splitting. Default is `0`.\n",
       "- `verbose` (bool, optional): Whether to print verbose output. Default is `True`.\n",
       "\n",
       "**Methods**:\n",
       "\n",
       "#### Method: `tokenize`\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "def tokenize(self, text, encoding_name=None, return_byte=False, **kwargs):\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `text` (str): The text to tokenize.\n",
       "- `encoding_name` (str, optional): The name of the encoding. Default is `None`.\n",
       "- `return_byte` (bool, optional): Whether to return bytes. Default is `False`.\n",
       "- `**kwargs`: Additional arguments for the tokenizer.\n",
       "\n",
       "**Return Values**:\n",
       "- The tokenized text.\n",
       "\n",
       "#### Method: `split`\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "def split(\n",
       "    self,\n",
       "    text,\n",
       "    chunk_size=None,\n",
       "    overlap=None,\n",
       "    threshold=None,\n",
       "    by_chars=False,\n",
       "    return_tokens=False,\n",
       "    return_byte=False,\n",
       "    **kwargs,\n",
       "):\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `text` (str): The text to split.\n",
       "- `chunk_size` (int, optional): The size of each chunk. Default is `None`.\n",
       "- `overlap` (int, optional): The number of overlapping tokens between chunks. Default is `None`.\n",
       "- `threshold` (int, optional): The threshold for splitting. Default is `None`.\n",
       "- `by_chars` (bool, optional): Whether to split by characters. Default is `False`.\n",
       "- `return_tokens` (bool, optional): Whether to return tokens. Default is `False`.\n",
       "- `return_byte` (bool, optional): Whether to return bytes. Default is `False`.\n",
       "- `**kwargs`: Additional arguments for the splitter.\n",
       "\n",
       "**Return Values**:\n",
       "- The split text.\n",
       "\n",
       "#### Method: `compress`\n",
       "\n",
       "**Signature**:\n",
       "```python\n",
       "async def compress(\n",
       "    self,\n",
       "    text,\n",
       "    target_ratio=None,\n",
       "    initial_text=None,\n",
       "    cumulative=False,\n",
       "    split_kwargs=None,\n",
       "    split_overlap=None,\n",
       "    split_threshold=None,\n",
       "    rank_by=\"perplexity\",\n",
       "    min_compression_score=None,\n",
       "    verbose=True,\n",
       "    **kwargs,\n",
       "):\n",
       "```\n",
       "\n",
       "**Parameters**:\n",
       "- `text` (str): The text to compress.\n",
       "- `target_ratio` (float, optional): The target compression ratio. Default is `None`.\n",
       "- `initial_text` (str, optional): The initial text to prepend. Default is `None`.\n",
       "- `cumulative` (bool, optional): Whether to use cumulative compression. Default is `False`.\n",
       "- `split_kwargs` (dict, optional): Additional arguments for splitting. Default is `None`.\n",
       "- `split_overlap` (int, optional): The number of overlapping tokens between chunks. Default is `None`.\n",
       "- `split_threshold` (int, optional): The threshold for splitting. Default is `None`.\n",
       "- `rank_by` (str, optional): The ranking method. Default is `\"perplexity\"`.\n",
       "- `min_compression_score` (float, optional): The minimum compression score. Default is `None`.\n",
       "- `verbose` (bool, optional): Whether to print verbose output. Default is `True`.\n",
       "- `**kwargs`: Additional arguments for the compression.\n",
       "\n",
       "**Return Values**:\n",
       "- The compressed text.\n",
       "\n",
       "**Usage Examples**:\n",
       "```python\n",
       "# Example 1: Basic usage with required parameters\n",
       "compressed_text = await llm_compressor.compress(\"This is a test text.\")\n",
       "print(compressed_text)\n",
       "\n",
       "# Example 2: Usage with target ratio and initial text\n",
       "compressed_text = await llm_compressor.compress(\"This is a test text.\", target_ratio=0.5, initial_text=\"Initial context.\")\n",
       "print(compressed_text)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check extension forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(form, \"extension_forms\"):\n",
    "    for i in form.extension_forms:\n",
    "        i.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The task invoked query engine 9 times.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "An imodel in lionagi is not explicitly defined in the provided information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The `to_list` function from `lion_core.libs` converts its input into a list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The `nget` function retrieves a value from a nested list or dictionary structure using a specified list of indices. These indices can be integers for lists and strings for dictionaries. If the target location cannot be reached or does not exist, the function returns a specified default value or raises a `LookupError` if no default is provided."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The purpose and functionality of the iModel class in the lionagi.os.operator.imodel.imodel module cannot be determined from the provided information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The provided code does not mention an 'iModel' class. Therefore, it is not possible to determine the purpose of such a class from the given information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The purpose of the 'nget' function is to retrieve a value from a nested list or dictionary structure using specified indices. It allows for the navigation through the nested structure with integers for lists and strings for dictionaries. If the target location cannot be reached or does not exist, the function returns a specified default value or raises a LookupError if no default is provided."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The purpose of the 'to_list' function in the provided code is to convert the given indices into a list format. This ensures that the indices can be uniformly processed, whether they were originally provided as a single element or multiple elements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "An imodel in lionagi is not explicitly defined in the provided information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The provided code context does not include definitions or explanations for 'imodel', 'to_list', 'nget', 'SysUtil', 'chunk_by_chars', 'chunk_by_tokens', or 'tokenize'. Therefore, their meanings cannot be determined from the given information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"The task invoked query engine {len(pile.query_response)} times.\")\n",
    "\n",
    "for i in pile.query_response:\n",
    "    display(Markdown(i.response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
